#!/usr/bin/env python3
"""
æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°AstrBotçŸ¥è¯†åº“çš„å¤–æŒ‚è„šæœ¬
æ”¯æŒ txtã€pdfã€wordã€excel ç­‰å¤šç§æ ¼å¼
è‡ªåŠ¨è®°å½•å·²ä¸Šä¼ æ–‡ä»¶ï¼Œé¿å…é‡å¤ä¸Šä¼ 
"""

import os
import json
import asyncio
import aiofiles
import aiohttp
from pathlib import Path
from typing import List, Dict, Any, Optional
import hashlib
from tqdm import tqdm
import argparse
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_upload.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchFileUploader:
    def __init__(self, 
                 astrbot_url: str = "http://localhost:6185", 
                 username: str = "Germania", 
                 password: str = None,
                 upload_record_file: str = "upload_record.json"):
        self.astrbot_url = astrbot_url.rstrip('/')
        self.username = username
        self.password = password or "a5a35ac207741709fda357b1d1e8ad31"
        self.session = None
        self.token = None
        self.upload_record_file = upload_record_file
        self.uploaded_files = self.load_upload_record()
        
    def load_upload_record(self) -> Dict[str, Dict]:
        """åŠ è½½å·²ä¸Šä¼ æ–‡ä»¶è®°å½•"""
        try:
            if os.path.exists(self.upload_record_file):
                with open(self.upload_record_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½ä¸Šä¼ è®°å½•å¤±è´¥: {e}")
        return {}
    
    def save_upload_record(self):
        """ä¿å­˜ä¸Šä¼ è®°å½•"""
        try:
            with open(self.upload_record_file, 'w', encoding='utf-8') as f:
                json.dump(self.uploaded_files, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸Šä¼ è®°å½•å¤±è´¥: {e}")
    
    def get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def is_file_uploaded(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ä¸Šä¼ """
        file_hash = self.get_file_hash(file_path)
        if not file_hash:
            return False
        
        file_name = os.path.basename(file_path)
        if file_name in self.uploaded_files:
            record = self.uploaded_files[file_name]
            return record.get('hash') == file_hash
        return False
    
    def mark_file_uploaded(self, file_path: str, collection_name: str):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²ä¸Šä¼ """
        file_name = os.path.basename(file_path)
        file_hash = self.get_file_hash(file_path)
        file_size = os.path.getsize(file_path)
        
        self.uploaded_files[file_name] = {
            'hash': file_hash,
            'size': file_size,
            'collection_name': collection_name,
            'upload_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'file_path': file_path
        }
        self.save_upload_record()
    
    async def login(self) -> bool:
        """ç™»å½•AstrBotç®¡ç†é¢æ¿"""
        self.session = aiohttp.ClientSession()
        login_data = {
            "username": self.username,
            "password": self.password
        }
        
        try:
            async with self.session.post(f"{self.astrbot_url}/api/auth/login", json=login_data) as response:
                if response.status == 200:
                    result = await response.json()
                    if result.get("status") == "ok":
                        self.token = result["data"]["token"]
                        logger.info("âœ… ç™»å½•æˆåŠŸ")
                        return True
                    else:
                        logger.error(f"âŒ ç™»å½•å¤±è´¥: {result.get('message')}")
                        return False
                else:
                    logger.error(f"âŒ ç™»å½•è¯·æ±‚å¤±è´¥: {response.status}")
                    return False
        except Exception as e:
            logger.error(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return False
    
    async def get_knowledge_bases(self) -> List[Dict]:
        """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
        headers = {"Authorization": f"Bearer {self.token}"}
        
        try:
            async with self.session.get(f"{self.astrbot_url}/api/plug/alkaid/kb/collections", headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    return result.get("data", [])
                else:
                    logger.warning(f"âš ï¸ è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"âŒ è·å–çŸ¥è¯†åº“åˆ—è¡¨å¼‚å¸¸: {e}")
            return []
    
    async def create_knowledge_base(self, name: str, description: str = "", emoji: str = "ğŸ“š") -> Optional[str]:
        """åˆ›å»ºçŸ¥è¯†åº“"""
        headers = {"Authorization": f"Bearer {self.token}"}
        
        # å…ˆè·å–embeddingæä¾›å•†
        try:
            async with self.session.get(f"{self.astrbot_url}/api/config/provider/list?provider_type=embedding", headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    providers = result.get("data", [])
                    if providers:
                        embedding_provider_id = providers[0]["id"]
                    else:
                        logger.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„embeddingæä¾›å•†")
                        return None
                else:
                    logger.error("âŒ è·å–æä¾›å•†åˆ—è¡¨å¤±è´¥")
                    return None
        except Exception as e:
            logger.error(f"âŒ è·å–æä¾›å•†åˆ—è¡¨å¼‚å¸¸: {e}")
            return None
        
        create_data = {
            "collection_name": name,
            "emoji": emoji,
            "description": description,
            "embedding_provider_id": embedding_provider_id
        }
        
        try:
            async with self.session.post(f"{self.astrbot_url}/api/plug/alkaid/kb/create_collection", 
                                       json=create_data, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    logger.info(f"åˆ›å»ºçŸ¥è¯†åº“å“åº”: {result}")  # è°ƒè¯•ä¿¡æ¯
                    if result.get("status") == "ok":
                        # è·å–åˆ›å»ºåçš„å®é™…çŸ¥è¯†åº“åç§°ï¼ˆå¯èƒ½åŒ…å«UUIDï¼‰
                        data = result.get("data", {})
                        if isinstance(data, dict):
                            created_name = data.get("collection_name", name)
                        else:
                            created_name = name
                        logger.info(f"âœ… åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ: {created_name}")
                        return created_name
                    else:
                        logger.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {result.get('message')}")
                        return None
                else:
                    logger.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“è¯·æ±‚å¤±è´¥: {response.status}")
                    return None
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºçŸ¥è¯†åº“å¼‚å¸¸: {e}")
            return None
    
    async def upload_file(self, file_path: str, collection_name: str, 
                         chunk_size: int = None, chunk_overlap: int = None) -> bool:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ°çŸ¥è¯†åº“"""
        headers = {"Authorization": f"Bearer {self.token}"}
        
        try:
            with open(file_path, 'rb') as f:
                data = aiohttp.FormData()
                data.add_field('file', f, filename=os.path.basename(file_path))
                data.add_field('collection_name', collection_name)
                
                if chunk_size:
                    data.add_field('chunk_size', str(chunk_size))
                if chunk_overlap:
                    data.add_field('chunk_overlap', str(chunk_overlap))
                
                async with self.session.post(f"{self.astrbot_url}/api/plug/alkaid/kb/collection/add_file", 
                                           data=data, headers=headers) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get("status") == "ok":
                            logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {os.path.basename(file_path)}")
                            self.mark_file_uploaded(file_path, collection_name)
                            return True
                        else:
                            logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {os.path.basename(file_path)} - {result.get('message')}")
                            return False
                    else:
                        logger.error(f"âŒ ä¸Šä¼ è¯·æ±‚å¤±è´¥: {os.path.basename(file_path)} - {response.status}")
                        return False
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ å¼‚å¸¸: {os.path.basename(file_path)} - {e}")
            return False
    
    def get_supported_files(self, folder_path: str) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶åˆ—è¡¨"""
        supported_extensions = {
            '.txt', '.pdf', '.doc', '.docx', '.xls', '.xlsx', 
            '.ppt', '.pptx', '.md', '.rtf', '.csv'
        }
        
        files = []
        try:
            for file_path in Path(folder_path).rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    files.append(str(file_path))
        except Exception as e:
            logger.error(f"âŒ æ‰«ææ–‡ä»¶å¤±è´¥: {e}")
        
        return sorted(files)
    
    async def close_session(self):
        """å…³é—­session"""
        if self.session:
            await self.session.close()
    
    async def batch_upload_files(self, folder_path: str, kb_name: str = "æ‰¹é‡ä¸Šä¼ çŸ¥è¯†åº“",
                               chunk_size: int = None, chunk_overlap: int = None,
                               delay: float = 1.0) -> bool:
        """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        try:
            if not await self.login():
                return False
            
            # æ£€æŸ¥æˆ–åˆ›å»ºçŸ¥è¯†åº“
            kbs = await self.get_knowledge_bases()
            kb_exists = any(kb.get('collection_name') == kb_name for kb in kbs)
            
            if not kb_exists:
                if not await self.create_knowledge_base(kb_name):
                    return False
                # ç­‰å¾…ä¸€ä¸‹è®©çŸ¥è¯†åº“åˆ›å»ºå®Œæˆ
                await asyncio.sleep(2)
                # é‡æ–°è·å–çŸ¥è¯†åº“åˆ—è¡¨ï¼Œæ‰¾åˆ°å®é™…åˆ›å»ºçš„çŸ¥è¯†åº“åç§°
                kbs = await self.get_knowledge_bases()
                logger.info(f"ğŸ“‹ çŸ¥è¯†åº“åˆ—è¡¨: {[kb.get('collection_name') for kb in kbs]}")
            
            # æ‰¾åˆ°åŒ¹é…çš„çŸ¥è¯†åº“ï¼ˆå¯èƒ½æ˜¯å¸¦UUIDçš„åç§°ï¼‰
            actual_kb_name = None
            for kb in kbs:
                if kb.get('collection_name') == kb_name or kb.get('collection_name', '').startswith(kb_name):
                    actual_kb_name = kb.get('collection_name')
                    break
            
            if not actual_kb_name:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŒ‡å®šçš„çŸ¥è¯†åº“ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„çŸ¥è¯†åº“
                if kbs:
                    actual_kb_name = kbs[0].get('collection_name')
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šçŸ¥è¯†åº“ '{kb_name}'ï¼Œä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“: {actual_kb_name}")
                else:
                    logger.error(f"âŒ æœªæ‰¾åˆ°çŸ¥è¯†åº“: {kb_name}")
                    logger.error(f"âŒ å¯ç”¨çŸ¥è¯†åº“: {[kb.get('collection_name') for kb in kbs]}")
                    return False
            
            logger.info(f"ğŸ“š ä½¿ç”¨çŸ¥è¯†åº“: {actual_kb_name}")
            
            # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
            files = self.get_supported_files(folder_path)
            if not files:
                logger.warning(f"âš ï¸ åœ¨ {folder_path} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶")
                return False
            
            logger.info(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
            
            # è¿‡æ»¤å·²ä¸Šä¼ çš„æ–‡ä»¶
            new_files = []
            skipped_files = []
            
            for file_path in files:
                if self.is_file_uploaded(file_path):
                    skipped_files.append(file_path)
                else:
                    new_files.append(file_path)
            
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"  - æ€»æ–‡ä»¶æ•°: {len(files)}")
            logger.info(f"  - å·²ä¸Šä¼ : {len(skipped_files)}")
            logger.info(f"  - å¾…ä¸Šä¼ : {len(new_files)}")
            
            if not new_files:
                logger.info("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²ä¸Šä¼ å®Œæˆ")
                return True
            
            # å¼€å§‹ä¸Šä¼ 
            success_count = 0
            failed_count = 0
            
            with tqdm(new_files, desc="ä¸Šä¼ è¿›åº¦", unit="æ–‡ä»¶") as pbar:
                for file_path in pbar:
                    pbar.set_description(f"æ­£åœ¨ä¸Šä¼ : {os.path.basename(file_path)}")
                    
                    if await self.upload_file(file_path, actual_kb_name, chunk_size, chunk_overlap):
                        success_count += 1
                    else:
                        failed_count += 1
                    
                    # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    if delay > 0:
                        await asyncio.sleep(delay)
            
            # è¾“å‡ºç»“æœ
            logger.info(f"ğŸ‰ æ‰¹é‡ä¸Šä¼ å®Œæˆ!")
            logger.info(f"  - æˆåŠŸ: {success_count}")
            logger.info(f"  - å¤±è´¥: {failed_count}")
            logger.info(f"  - è·³è¿‡: {len(skipped_files)}")
            
            return failed_count == 0
        finally:
            await self.close_session()

async def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ°AstrBotçŸ¥è¯†åº“")
    parser.add_argument("folder_path", help="è¦ä¸Šä¼ çš„æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--kb-name", default="æ‰¹é‡ä¸Šä¼ çŸ¥è¯†åº“", help="çŸ¥è¯†åº“åç§°")
    parser.add_argument("--astrbot-url", default="http://localhost:6185", help="AstrBotåœ°å€")
    parser.add_argument("--username", default="Germania", help="ç”¨æˆ·å")
    parser.add_argument("--password", help="å¯†ç ")
    parser.add_argument("--chunk-size", type=int, help="åˆ†ç‰‡å¤§å°")
    parser.add_argument("--chunk-overlap", type=int, help="åˆ†ç‰‡é‡å ")
    parser.add_argument("--delay", type=float, default=1.0, help="ä¸Šä¼ é—´éš”(ç§’)")
    parser.add_argument("--record-file", default="upload_record.json", help="ä¸Šä¼ è®°å½•æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.folder_path):
        logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.folder_path}")
        return
    
    # åˆ›å»ºä¸Šä¼ å™¨
    uploader = BatchFileUploader(
        astrbot_url=args.astrbot_url,
        username=args.username,
        password=args.password,
        upload_record_file=args.record_file
    )
    
    # å¼€å§‹æ‰¹é‡ä¸Šä¼ 
    success = await uploader.batch_upload_files(
        folder_path=args.folder_path,
        kb_name=args.kb_name,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        delay=args.delay
    )
    
    if success:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆ!")
    else:
        logger.error("âŒ éƒ¨åˆ†æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    asyncio.run(main()) 